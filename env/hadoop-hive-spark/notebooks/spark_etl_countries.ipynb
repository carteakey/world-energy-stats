{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f543c6-fc4e-419f-97b2-926a95537231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"world-energy-stats\") \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Local Development\n",
    "# spark = SparkSession.builder.appName(\"world-energy-stats\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfbf750-99a1-42f9-9812-d0f5142d7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "   .load(\"hdfs://namenode:9000/energy-data/owid-energy-data.csv\"))\n",
    "\n",
    "#Local Development\n",
    "# df = spark.read.csv(\"owid-energy-data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e216a-816b-43cf-9fc8-9ce2e6b3b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROPPING REGIONS (FOR NOW)\n",
    "df = df.filter(df['iso_code'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc992f-2a4e-4cc4-ade6-af50e453ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['year'] >=1990]\n",
    "\n",
    "# Drop 2022 as well.\n",
    "# df = df[df['year'] >=1990]\n",
    "\n",
    "#40 years of data\n",
    "grouped_df = df.groupBy(\"year\").count().orderBy(\"year\")\n",
    "grouped_df.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b263e1f-7b78-4c65-9ce4-acca1782673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns\n",
    "cols_to_drop = [col for col in df.columns if '_per_gdp' if '_per_capita' in col or '_change_pct' in col or '_change_twh' in col]\n",
    "df = df.drop(*cols_to_drop)\n",
    "# per_capita_electricity\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c54bd4-0a0f-46e8-ae53-1d7b1047d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last, first\n",
    "\n",
    "temp_column = [column for column in df.columns if 'year' not in column]\n",
    "temp_column = [column for column in temp_column if 'country' not in column]\n",
    "temp_column\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Define the windows for forward fill and backward fill\n",
    "ffill_window = \"(partition by country order by year rows between unbounded preceding and current row)\"\n",
    "# bfill_window = \"(partition by country order by year rows between current row and unbounded following)\"\n",
    "\n",
    "for col in temp_column:\n",
    "    df = (df.withColumn(col, F.expr(f\"case when isnan({col}) then null else {col} end\"))\n",
    "    .withColumn(col, F.expr(f\"coalesce({col}, last({col}, true) over {ffill_window})\")))\n",
    "    # .withColumn(col, F.expr(f\"coalesce({col}, first({col}, true) over {bfill_window})\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d678c1-2761-48e3-8faf-f2813eb32a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LEVEL 1 CATEGORIZATION FOR BACKFILLING AND LOGICAL SEPARATION\n",
    "\n",
    "# Primary Key Columns\n",
    "primary_keys = ['country', 'year', 'iso_code']\n",
    "\n",
    "# 1. General Information\n",
    "df_general = df[primary_keys + ['population', 'gdp', 'electricity_demand', 'electricity_generation', 'primary_energy_consumption']]\n",
    "\n",
    "# 2. Biofuel\n",
    "df_biofuel = df[primary_keys + ['biofuel_consumption', 'biofuel_electricity', 'biofuel_share_elec', 'biofuel_share_energy']]\n",
    "\n",
    "# 3. Coal\n",
    "df_coal = df[primary_keys + ['coal_consumption', 'coal_electricity', 'coal_production', 'coal_share_elec', 'coal_share_energy']]\n",
    "\n",
    "# 4. Gas\n",
    "df_gas = df[primary_keys + ['gas_consumption', 'gas_electricity', 'gas_production', 'gas_share_elec', 'gas_share_energy']]\n",
    "\n",
    "# 5. Oil\n",
    "df_oil = df[primary_keys + ['oil_consumption', 'oil_electricity', 'oil_production', 'oil_share_elec', 'oil_share_energy']]\n",
    "\n",
    "# 6. Fossil Fuels (Aggregate)\n",
    "df_fossil = df[primary_keys + ['fossil_electricity', 'fossil_fuel_consumption', 'fossil_share_elec', 'fossil_share_energy', 'carbon_intensity_elec']]\n",
    "\n",
    "# 7. Greenhouse Gas\n",
    "df_greenhouse_gas = df[primary_keys + ['greenhouse_gas_emissions']]\n",
    "\n",
    "# 8. Hydro\n",
    "df_hydro = df[primary_keys + ['hydro_consumption', 'hydro_electricity', 'hydro_share_elec', 'hydro_share_energy']]\n",
    "\n",
    "# 9. Nuclear\n",
    "df_nuclear = df[primary_keys + ['nuclear_consumption', 'nuclear_electricity', 'nuclear_share_elec', 'nuclear_share_energy']]\n",
    "\n",
    "# 10. Renewables (Aggregate)\n",
    "df_renewables = df[primary_keys + ['renewables_consumption', 'renewables_electricity', 'renewables_share_elec', 'renewables_share_energy']]\n",
    "\n",
    "# 11. Solar\n",
    "df_solar = df[primary_keys + ['solar_consumption', 'solar_electricity', 'solar_share_elec', 'solar_share_energy']]\n",
    "\n",
    "# 12. Wind\n",
    "df_wind = df[primary_keys + ['wind_consumption', 'wind_electricity', 'wind_share_elec', 'wind_share_energy']]\n",
    "\n",
    "# 13. Other Renewables\n",
    "df_other_renewables = df[primary_keys + ['other_renewable_consumption', 'other_renewable_electricity', 'other_renewable_exc_biofuel_electricity', 'other_renewables_share_elec', 'other_renewables_share_elec_exc_biofuel', 'other_renewables_share_energy']]\n",
    "\n",
    "# 14. Low Carbon\n",
    "df_low_carbon = df[primary_keys + ['low_carbon_consumption', 'low_carbon_electricity', 'low_carbon_share_elec', 'low_carbon_share_energy']]\n",
    "\n",
    "# 15. Electricity Imports\n",
    "df_electricity_imports = df[primary_keys + ['net_elec_imports', 'net_elec_imports_share_demand']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74a438-f5af-4836-b210-21aa25f44d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def filter_df_by_threshold(df, threshold):\n",
    "    \"\"\"\n",
    "    Filter a dataframe based on the threshold of non-null counts in non-primary columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input dataframe.\n",
    "    - threshold: The minimum number of non-null values required across non-primary columns.\n",
    "\n",
    "    Returns:\n",
    "    - filtered_df: The filtered dataframe.\n",
    "    - stats: A dictionary containing statistics about the filtering process.\n",
    "    \"\"\"\n",
    "\n",
    "    # Primary Key Columns\n",
    "    primary_keys = ['country', 'year', 'iso_code']\n",
    "\n",
    "    # List of columns to check for null values\n",
    "    columns_to_check = [col for col in df.columns if col not in primary_keys]\n",
    "\n",
    "    # Count non-null values across all non-primary columns for each country\n",
    "    agg_exprs = [F.count(F.when(F.col(c).isNotNull(), 1)).alias(c + '_non_null_count') for c in columns_to_check]\n",
    "    country_counts = df.groupBy('country').agg(*agg_exprs)\n",
    "\n",
    "    # Sum the non-null counts across all columns for each country\n",
    "    total_non_null_counts = sum(F.col(c + '_non_null_count') for c in columns_to_check)\n",
    "    country_counts = country_counts.withColumn('total_non_null_counts', total_non_null_counts)\n",
    "    \n",
    "   # Filter countries based on the threshold\n",
    "    countries_to_keep_df = country_counts.filter(F.col('total_non_null_counts') > threshold).select('country')\n",
    "\n",
    "    # Find out the countries that were dropped\n",
    "    all_countries = df.select('country').distinct()\n",
    "    dropped_countries_df = all_countries.subtract(countries_to_keep_df)\n",
    "    dropped_countries = [row['country'] for row in dropped_countries_df.collect()]\n",
    "\n",
    "    # Join with the original DataFrame to get the filtered data\n",
    "    filtered_df = df.join(countries_to_keep_df, on='country', how='inner')\n",
    "\n",
    "    original_row_count = df.count()\n",
    "    filtered_row_count = filtered_df.count()\n",
    "    rows_dropped = original_row_count - filtered_row_count\n",
    "\n",
    "    stats = {\n",
    "        'Original number of rows': original_row_count,\n",
    "        'Number of rows after filtering': filtered_row_count,\n",
    "        'Number of rows dropped': rows_dropped,\n",
    "        'Dropped countries': dropped_countries\n",
    "    }\n",
    "    \n",
    "    print(stats)\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# # Usage example:\n",
    "# filtered_df_fossil = filter_df_by_threshold(df_fossil, 5)\n",
    "\n",
    "def count_nulls_by_country(df):\n",
    "    \"\"\"\n",
    "    Count the number of null values for each country and each column (except 'country').\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input dataframe.\n",
    "\n",
    "    Returns:\n",
    "    - null_counts_df: A dataframe with the count of null values for each column and country.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the aggregation expressions\n",
    "    agg_exprs = [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns if c != 'country']\n",
    "\n",
    "    # Group by 'country' and aggregate\n",
    "    null_counts_df = df.groupBy(\"country\").agg(*agg_exprs)\n",
    "\n",
    "    return null_counts_df\n",
    "\n",
    "# # Usage example:\n",
    "# null_counts_fossil = count_nulls_by_country(df_fossil)\n",
    "\n",
    "# # Show the results\n",
    "# null_counts_fossil.show(n=300)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def filter_rows_by_null_threshold(df):\n",
    "    \"\"\"\n",
    "    Filter rows from a dataframe based on the threshold of null values across non-primary columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input dataframe.\n",
    "\n",
    "    Returns:\n",
    "    - filtered_df: The filtered dataframe.\n",
    "    - stats: A dictionary containing statistics about the filtering process.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Primary Key Columns\n",
    "    primary_keys = ['country', 'year', 'iso_code']\n",
    "\n",
    "    # List of columns to check for null values\n",
    "    columns_to_check = [col for col in df.columns if col not in primary_keys]\n",
    "\n",
    "    # Set the threshold equal to the number of non-primary key columns\n",
    "    threshold = len(columns_to_check)\n",
    "\n",
    "    # Calculate the number of nulls for each row\n",
    "    null_count = sum(F.when(F.col(c).isNull(), 1).otherwise(0) for c in columns_to_check)\n",
    "\n",
    "    # Filter rows based on the threshold\n",
    "    filtered_df = df.filter(null_count < threshold)\n",
    "\n",
    "    original_row_count = df.count()\n",
    "    filtered_row_count = filtered_df.count()\n",
    "    rows_dropped = original_row_count - filtered_row_count\n",
    "\n",
    "    stats = {\n",
    "        'Original number of rows': original_row_count,\n",
    "        'Number of rows after filtering': filtered_row_count,\n",
    "        'Number of rows dropped': rows_dropped\n",
    "    }\n",
    "    \n",
    "    print(stats)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# # Usage example:\n",
    "# filtered_df_fossil, fossil_stats = filter_rows_by_null_threshold(df_fossil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf12623b-7ce9-4238-a3ac-5aa6e6a142ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "null_counts_fossil = count_nulls_by_country(df_fossil)\n",
    "\n",
    "# # Show the results\n",
    "null_counts_fossil.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b603e-c9b5-4dee-a9e3-9c73890889c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the filter function on each dataframe\n",
    "filtered_df_general = filter_df_by_threshold(df_general, 0)\n",
    "filtered_df_biofuel = filter_df_by_threshold(df_biofuel, 0)\n",
    "filtered_df_coal = filter_df_by_threshold(df_coal, 0)\n",
    "filtered_df_gas = filter_df_by_threshold(df_gas, 0)\n",
    "filtered_df_oil = filter_df_by_threshold(df_oil, 0)\n",
    "filtered_df_fossil = filter_df_by_threshold(df_fossil, 0)\n",
    "filtered_df_greenhouse_gas = filter_df_by_threshold(df_greenhouse_gas, 0)\n",
    "filtered_df_hydro = filter_df_by_threshold(df_hydro, 0)\n",
    "filtered_df_nuclear = filter_df_by_threshold(df_nuclear, 0)\n",
    "filtered_df_renewables = filter_df_by_threshold(df_renewables, 0)\n",
    "filtered_df_solar = filter_df_by_threshold(df_solar, 0)\n",
    "filtered_df_wind = filter_df_by_threshold(df_wind, 0)\n",
    "filtered_df_other_renewables = filter_df_by_threshold(df_other_renewables, 0)\n",
    "filtered_df_low_carbon = filter_df_by_threshold(df_low_carbon, 0)\n",
    "filtered_df_electricity_imports = filter_df_by_threshold(df_electricity_imports, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e60749-1746-4b07-a816-505f887d9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_ren = filter_df_by_threshold(df_renewables, 0)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "null_counts_ren = count_nulls_by_country(filtered_df_ren)\n",
    "\n",
    "# Show the results\n",
    "null_counts_ren.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf36bcc-b623-4401-ab39-ec3b20636778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path for saving the CSV files\n",
    "folder_path = './clean/'\n",
    "\n",
    "# Define file paths for each dataframe within the \"clean\" folder\n",
    "filtered_df_general.toPandas().to_csv(folder_path + 'general.csv', index=False)\n",
    "filtered_df_biofuel.toPandas().to_csv(folder_path + 'biofuel.csv', index=False)\n",
    "filtered_df_coal.toPandas().to_csv(folder_path + 'coal.csv', index=False)\n",
    "filtered_df_gas.toPandas().to_csv(folder_path + 'gas.csv', index=False)\n",
    "filtered_df_oil.toPandas().to_csv(folder_path + 'oil.csv', index=False)\n",
    "filtered_df_fossil.toPandas().to_csv(folder_path + 'fossil.csv', index=False)\n",
    "filtered_df_greenhouse_gas.toPandas().to_csv(folder_path + 'greenhouse_gas.csv', index=False)\n",
    "filtered_df_hydro.toPandas().to_csv(folder_path + 'hydro.csv', index=False)\n",
    "filtered_df_nuclear.toPandas().to_csv(folder_path + 'nuclear.csv', index=False)\n",
    "filtered_df_renewables.toPandas().to_csv(folder_path + 'renewables.csv', index=False)\n",
    "filtered_df_solar.toPandas().to_csv(folder_path + 'solar.csv', index=False)\n",
    "filtered_df_wind.toPandas().to_csv(folder_path + 'wind.csv', index=False)\n",
    "filtered_df_other_renewables.toPandas().to_csv(folder_path + 'other_renewables.csv', index=False)\n",
    "filtered_df_low_carbon.toPandas().to_csv(folder_path + 'low_carbon.csv', index=False)\n",
    "filtered_df_electricity_imports.toPandas().to_csv(folder_path + 'electricity_imports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4d16c-b710-40fc-955c-862eb478b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to hive tables.\n",
    "filtered_df_general.write.mode(\"overwrite\").saveAsTable(\"wes.general\")\n",
    "filtered_df_biofuel.write.mode(\"overwrite\").saveAsTable(\"wes.biofuel\")\n",
    "filtered_df_coal.write.mode(\"overwrite\").saveAsTable(\"wes.coal\")\n",
    "filtered_df_gas.write.mode(\"overwrite\").saveAsTable(\"wes.gas\")\n",
    "filtered_df_oil.write.mode(\"overwrite\").saveAsTable(\"wes.oil\")\n",
    "filtered_df_fossil.write.mode(\"overwrite\").saveAsTable(\"wes.fossil\")\n",
    "filtered_df_greenhouse_gas.write.mode(\"overwrite\").saveAsTable(\"wes.greenhouse_gas\")\n",
    "filtered_df_hydro.write.mode(\"overwrite\").saveAsTable(\"wes.hydro\")\n",
    "filtered_df_nuclear.write.mode(\"overwrite\").saveAsTable(\"wes.nuclear\")\n",
    "filtered_df_renewables.write.mode(\"overwrite\").saveAsTable(\"wes.renewables\")\n",
    "filtered_df_solar.write.mode(\"overwrite\").saveAsTable(\"wes.solar\")\n",
    "filtered_df_wind.write.mode(\"overwrite\").saveAsTable(\"wes.wind\")\n",
    "filtered_df_other_renewables.write.mode(\"overwrite\").saveAsTable(\"wes.other_renewables\")\n",
    "filtered_df_low_carbon.write.mode(\"overwrite\").saveAsTable(\"wes.low_carbon\")\n",
    "filtered_df_electricity_imports.write.mode(\"overwrite\").saveAsTable(\"wes.electricity_imports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb8721ae-fa63-4dbc-adac-7e987a7234b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3884440000.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_396/3884440000.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    SELECT * FROM wes.renewables;\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "SELECT * FROM wes.renewables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e8bbc-5e54-481f-a4c0-c81f8de297c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
